{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Читаем CSV с обработкой больших файлов...\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n",
      "Чанк добавлен\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Пути к файлам\n",
    "csv_path = 'data/new-york-city-taxi-fare-prediction/train.csv'\n",
    "feather_path = 'data/new-york-city-taxi-fare-prediction/train_25000000.feather'\n",
    "\n",
    "# Читаем CSV с обработкой больших файлов\n",
    "print(\"Читаем CSV с обработкой больших файлов...\")\n",
    "chunk_size = 1_000_000  # Читаем по 1 млн строк за раз\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(csv_path, chunksize=chunk_size, nrows=25_000_000):\n",
    "    chunks.append(chunk)\n",
    "    print(\"Чанк добавлен\")\n",
    "\n",
    "# Объединяем части и сохраняем в Feather\n",
    "print(\"Объединяем части и сохраняем в Feather...\")\n",
    "df = pd.concat(chunks)\n",
    "df.to_feather(feather_path)\n",
    "\n",
    "print(f\"Файл сохранен в {feather_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/repos/jupiter/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import urllib\n",
    "import PIL\n",
    "import requests\n",
    "from datetime import date\n",
    "import holidays\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "random_state = np.random.RandomState(714)\n",
    "rng = np.random.default_rng(714)\n",
    "\n",
    "def select_within_boundary(df, boundary) -> bool:\n",
    "    return (\n",
    "        (df[\"pickup_longitude\"] >= boundary[\"longitude_min\"])\n",
    "        & (df[\"pickup_longitude\"] <= boundary[\"longitude_max\"])\n",
    "        & (df[\"pickup_latitude\"] >= boundary[\"latitude_min\"])\n",
    "        & (df[\"pickup_latitude\"] <= boundary[\"latitude_max\"])\n",
    "        & (df[\"dropoff_longitude\"] >= boundary[\"longitude_min\"])\n",
    "        & (df[\"dropoff_longitude\"] <= boundary[\"longitude_max\"])\n",
    "        & (df[\"dropoff_latitude\"] >= boundary[\"latitude_min\"])\n",
    "        & (df[\"dropoff_latitude\"] <= boundary[\"latitude_max\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def select_in_boundary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    boundary = {\n",
    "        \"longitude_min\": -74.5,\n",
    "        \"longitude_max\": -72.8,\n",
    "        \"latitude_min\": 40.5,\n",
    "        \"latitude_max\": 41.8,\n",
    "    }\n",
    "\n",
    "    return df[select_within_boundary(df, boundary)]\n",
    "\n",
    "\n",
    "def drop_on_water(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def lonlat_to_xy(longitude, latitude, x_range, y_range, boundary):\n",
    "        longitude_range = boundary[\"longitude_max\"] - boundary[\"longitude_min\"]\n",
    "        latitude_range = boundary[\"latitude_max\"] - boundary[\"latitude_min\"]\n",
    "\n",
    "        x = x_range * (longitude - boundary[\"longitude_min\"]) / longitude_range\n",
    "        y = (\n",
    "            y_range\n",
    "            - y_range * (latitude - boundary[\"latitude_min\"]) / latitude_range\n",
    "        )\n",
    "\n",
    "        return (x.astype(int), y.astype(int))\n",
    "\n",
    "    mask_url = urllib.request.urlopen(\"https://imgur.com/XGHkdoK.png\")\n",
    "    mask = np.array(PIL.Image.open(mask_url))[:, :, 0] > 0.92\n",
    "\n",
    "    mask = np.c_[mask, np.full([mask.shape[0], 1], False)]\n",
    "    mask = np.r_[mask, np.full([1, mask.shape[1]], False)]\n",
    "\n",
    "    boundary = {\n",
    "        \"longitude_min\": -74.5,\n",
    "        \"longitude_max\": -72.8,\n",
    "        \"latitude_min\": 40.5,\n",
    "        \"latitude_max\": 41.8,\n",
    "    }\n",
    "\n",
    "    pickup_x, pickup_y = lonlat_to_xy(\n",
    "        df.loc[:, \"pickup_longitude\"],\n",
    "        df.loc[:, \"pickup_latitude\"],\n",
    "        mask.shape[1] - 1,\n",
    "        mask.shape[0] - 1,\n",
    "        boundary,\n",
    "    )\n",
    "\n",
    "    dropoff_x, dropoff_y = lonlat_to_xy(\n",
    "        df.loc[:, \"dropoff_longitude\"],\n",
    "        df.loc[:, \"dropoff_latitude\"],\n",
    "        mask.shape[1] - 1,\n",
    "        mask.shape[0] - 1,\n",
    "        boundary,\n",
    "    )\n",
    "\n",
    "    on_land = mask[pickup_y, pickup_x] & mask[dropoff_y, dropoff_x]\n",
    "    return df[on_land]\n",
    "\n",
    "\n",
    "def drop_same_pick_drop(df: pd.DataFrame):\n",
    "    filter = (df[\"pickup_longitude\"] == df[\"dropoff_longitude\"]) & (\n",
    "        df[\"pickup_latitude\"] == df[\"dropoff_latitude\"]\n",
    "    )\n",
    "\n",
    "    return df[~filter]\n",
    "\n",
    "\n",
    "def drop_nonsense_fareamount(df: pd.DataFrame):\n",
    "    return df[(df.fare_amount > 0) & (df.fare_amount <= 500)]\n",
    "\n",
    "\n",
    "def get_lat_lon(df: pd.DataFrame, unit=\"rad\"):\n",
    "    # Return lat, lon in radian\n",
    "    lat1 = df[\"pickup_latitude\"].copy().to_numpy()\n",
    "    lon1 = df[\"pickup_longitude\"].copy().to_numpy()\n",
    "    lat2 = df[\"dropoff_latitude\"].copy().to_numpy()\n",
    "    lon2 = df[\"dropoff_longitude\"].copy().to_numpy()\n",
    "\n",
    "    return lat1, lon1, lat2, lon2\n",
    "\n",
    "\n",
    "def cal_rotated_coordinate(lat1, lon1, lat2, lon2) -> np.ndarray:\n",
    "    p1 = np.column_stack([lat1, lon1])\n",
    "    p2 = np.column_stack([lat2, lon2])\n",
    "\n",
    "    theta = -np.radians(29).astype(\"float32\")\n",
    "\n",
    "    rot = [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]]\n",
    "\n",
    "    # Perform rotate row by row and split\n",
    "    lat1, lon1 = np.hsplit(np.einsum(\"ij, mj -> mi\", rot, p1), 2)\n",
    "    lat2, lon2 = np.hsplit(np.einsum(\"ij, mj -> mi\", rot, p2), 2)\n",
    "    lat1, lon1, lat2, lon2 = map(lambda x: x.ravel(), [lat1, lon1, lat2, lon2])\n",
    "    return lat1, lon1, lat2, lon2\n",
    "\n",
    "\n",
    "def get_rotated_coordinate(df: pd.DataFrame):\n",
    "    lat1, lon1, lat2, lon2 = get_lat_lon(df)\n",
    "    header = [\n",
    "        \"rotated_pickup_latitude\",\n",
    "        \"rotated_pickup_longitude\",\n",
    "        \"rotated_dropoff_latitude\",\n",
    "        \"rotated_dropoff_longitude\",\n",
    "    ]\n",
    "\n",
    "    mtx = cal_rotated_coordinate(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    df_coordinate = pd.DataFrame(mtx, index=header).transpose()\n",
    "\n",
    "    return pd.concat([df, df_coordinate], axis=1)\n",
    "\n",
    "\n",
    "def get_euclidean(df: pd.DataFrame):\n",
    "    lat1, lon1, lat2, lon2 = get_lat_lon(df)\n",
    "    return (\n",
    "        np.linalg.norm(\n",
    "            np.column_stack([lat1, lon1]) - np.column_stack([lat2, lon2]),\n",
    "            axis=1,\n",
    "        )\n",
    "        * 3959\n",
    "    )\n",
    "\n",
    "\n",
    "def cal_haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    dlat = lat1 - lat2\n",
    "    dlon = lon1 - lon2\n",
    "\n",
    "    tmp = (\n",
    "        np.sin(dlat / 2.0) ** 2\n",
    "        + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    )\n",
    "\n",
    "    return 2 * np.arcsin(np.sqrt(tmp)) * 3959\n",
    "\n",
    "\n",
    "def get_haversine_distance(df: pd.DataFrame):\n",
    "    # Return haversine distance in miles\n",
    "    lat1, lon1, lat2, lon2 = get_lat_lon(df)\n",
    "    return cal_haversine_distance(lat1, lon1, lat2, lon2)\n",
    "\n",
    "\n",
    "def get_correct_manhattan(df: pd.DataFrame):\n",
    "    lat1, lon1, lat2, lon2 = get_lat_lon(df, \"mile\")\n",
    "    lat1, lon1, lat2, lon2 = cal_rotated_coordinate(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    dlat = abs(lat1 - lat2)\n",
    "    dlon = abs(lon1 - lon2)\n",
    "\n",
    "    return (dlat + dlon).ravel()\n",
    "\n",
    "\n",
    "def get_haversine_bearing(df: pd.DataFrame):\n",
    "    lat1, lon1, lat2, lon2 = get_lat_lon(df)\n",
    "\n",
    "    dlat = lat1 - lat2\n",
    "    dlon = lon1 - lon2\n",
    "\n",
    "    return np.arctan2(\n",
    "        np.sin(dlon * np.cos(lat2)),\n",
    "        np.cos(lat1) * np.sin(lat2)\n",
    "        - np.sin(lat1) * np.cos(lat2) * np.cos(dlon),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_historical_temp_precipitation():\n",
    "    url = f\"https://archive-api.open-meteo.com/v1/archive?latitude=40.71&longitude=-74.01&start_date=2009-01-01&end_date=2015-12-31&hourly=apparent_temperature,precipitation\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    df_tmp = pd.DataFrame(data[\"hourly\"])\n",
    "    df_tmp[\"time\"] = pd.to_datetime(df_tmp[\"time\"])\n",
    "\n",
    "    return df_tmp.set_index(\"time\").to_dict()\n",
    "\n",
    "\n",
    "def convert_time(x: pd.Series) -> pd.Series:\n",
    "    date = pd.to_datetime(x.dt.date)\n",
    "    hour = x.dt.hour\n",
    "\n",
    "    return pd.Series(date + hour.astype(\"timedelta64[h]\"))\n",
    "\n",
    "\n",
    "def add_temp_precipitation(df: pd.DataFrame):\n",
    "    temp_dict = get_historical_temp_precipitation()\n",
    "    date_time = convert_time(df[\"pickup_datetime\"])\n",
    "    df[\"apparent_temperature\"] = date_time.map(\n",
    "        temp_dict[\"apparent_temperature\"]\n",
    "    ).astype(\"float32\")\n",
    "    df[\"precipitation\"] = date_time.map(temp_dict[\"precipitation\"]).astype(\n",
    "        \"float32\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# add time information\n",
    "def add_time_and_holiday_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Add time information\n",
    "    df[\"year\"] = df.pickup_datetime.dt.year\n",
    "    df[\"month\"] = df.pickup_datetime.dt.month.astype(\"uint8\")\n",
    "    df[\"day\"] = df.pickup_datetime.dt.day.astype(\"uint8\")\n",
    "    df[\"weekday\"] = df.pickup_datetime.dt.weekday.astype(\"uint8\")\n",
    "    df[\"hour\"] = df.pickup_datetime.dt.hour.astype(\"uint8\")\n",
    "\n",
    "    # Add holiday information\n",
    "#     us_holidays = holidays.US()\n",
    "#     df[\"is_holiday\"] = df.pickup_datetime.dt.date.isin(us_holidays).astype(\n",
    "#         \"uint8\"\n",
    "#     )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def distance_to_airport(df):\n",
    "    \"\"\"\n",
    "    Return minumum distance from pickup or dropoff coordinates to each airport.\n",
    "    JFK: John F. Kennedy International Airport\n",
    "    EWR: Newark Liberty International Airport\n",
    "    LGA: LaGuardia Airport\n",
    "    SOL: Statue of Liberty\n",
    "    NYC: Newyork Central\n",
    "    \"\"\"\n",
    "    jfk_coord = np.radians((40.639722, -73.778889))\n",
    "    ewr_coord = np.radians((40.6925, -74.168611))\n",
    "    lga_coord = np.radians((40.77725, -73.872611))\n",
    "    sol_coord = np.radians((40.6892, -74.0445))  # Statue of Liberty\n",
    "    nyc_coord = np.radians((40.7141667, -74.0063889))\n",
    "\n",
    "    pickup_lat = df[\"pickup_latitude\"]\n",
    "    dropoff_lat = df[\"dropoff_latitude\"]\n",
    "    pickup_lon = df[\"pickup_longitude\"]\n",
    "    dropoff_lon = df[\"dropoff_longitude\"]\n",
    "\n",
    "    pickup_jfk = cal_haversine_distance(\n",
    "        pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]\n",
    "    )\n",
    "    dropoff_jfk = cal_haversine_distance(\n",
    "        jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon\n",
    "    )\n",
    "    pickup_ewr = cal_haversine_distance(\n",
    "        pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1]\n",
    "    )\n",
    "    dropoff_ewr = cal_haversine_distance(\n",
    "        ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon\n",
    "    )\n",
    "    pickup_lga = cal_haversine_distance(\n",
    "        pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]\n",
    "    )\n",
    "    dropoff_lga = cal_haversine_distance(\n",
    "        lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon\n",
    "    )\n",
    "    pickup_sol = cal_haversine_distance(\n",
    "        pickup_lat, pickup_lon, sol_coord[0], sol_coord[1]\n",
    "    )\n",
    "    dropoff_sol = cal_haversine_distance(\n",
    "        sol_coord[0], sol_coord[1], dropoff_lat, dropoff_lon\n",
    "    )\n",
    "    pickup_nyc = cal_haversine_distance(\n",
    "        pickup_lat, pickup_lon, nyc_coord[0], nyc_coord[1]\n",
    "    )\n",
    "    dropoff_nyc = cal_haversine_distance(\n",
    "        nyc_coord[0], nyc_coord[1], dropoff_lat, dropoff_lon\n",
    "    )\n",
    "\n",
    "    df[\"jfk_dist\"] = (pickup_jfk + dropoff_jfk).astype(\"float32\")\n",
    "    df[\"ewr_dist\"] = (pickup_ewr + dropoff_ewr).astype(\"float32\")\n",
    "    df[\"lga_dist\"] = (pickup_lga + dropoff_lga).astype(\"float32\")\n",
    "    df[\"sol_dist\"] = (pickup_sol + dropoff_sol).astype(\"float32\")\n",
    "    df[\"nyc_dist\"] = (pickup_nyc + dropoff_nyc).astype(\"float32\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def date_format(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    date_time = df[\"pickup_datetime\"].copy()\n",
    "\n",
    "    date_time = date_time.str.slice(0, 16)\n",
    "    date_time = pd.to_datetime(date_time, utc=True, format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    df[\"pickup_datetime\"] = date_time\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Drop negative fare amount\n",
    "    df = drop_nonsense_fareamount(df)\n",
    "    # Drop nan value\n",
    "    df = df.dropna()\n",
    "    # Drop data out of boundary\n",
    "    df = select_in_boundary(df)\n",
    "    # Drop data on water\n",
    "    df = drop_on_water(df)\n",
    "    # Drop same pickup and dropoff data\n",
    "    # df = drop_same_pick_drop(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineering(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    # Degree to radian\n",
    "    df.loc[:, \"pickup_longitude\"] = np.radians(df.loc[:, \"pickup_longitude\"])\n",
    "    df.loc[:, \"pickup_latitude\"] = np.radians(df.loc[:, \"pickup_latitude\"])\n",
    "    df.loc[:, \"dropoff_longitude\"] = np.radians(df.loc[:, \"dropoff_longitude\"])\n",
    "    df.loc[:, \"dropoff_latitude\"] = np.radians(df.loc[:, \"dropoff_latitude\"])\n",
    "    # Add rotate coordinate\n",
    "#     df = get_rotated_coordinate(df)\n",
    "    # Add distance\n",
    "#     df.loc[:, \"euclidean\"] = get_euclidean(df)\n",
    "    df.loc[:, \"haversine\"] = get_haversine_distance(df)\n",
    "    df.loc[:, \"haversine_bearing\"] = get_haversine_bearing(df)\n",
    "#     df.loc[:, \"correct_manhattan\"] = get_correct_manhattan(df)\n",
    "    # Add Temp and precipitation\n",
    "#     df = add_temp_precipitation(df)\n",
    "    # Split time and add holiday\n",
    "    df = add_time_and_holiday_info(df)\n",
    "    # Add pickup/dropoff airport\n",
    "    df = distance_to_airport(df)\n",
    "    df = df.drop(columns=[\"pickup_datetime\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "train_path = 'data/new-york-city-taxi-fare-prediction/train_20000000.feather'\n",
    "test_path = 'data/new-york-city-taxi-fare-prediction/test.csv'\n",
    "key_path = 'data/new-york-city-taxi-fare-prediction/test.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# df_key = pd.read_csv(key_path, usecols=[\"key\"])\n",
    "data_train = pd.read_feather(train_path)[:100000]\n",
    "data_test = pd.read_csv(test_path)\n",
    "\n",
    "df_train = clean_data(data_train)\n",
    "df_train = engineering(df_train)\n",
    "df_test = engineering(data_test)\n",
    "\n",
    "# del train_path, test_path, key_path, data_train\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "cat_feature = [\"year\", \"month\", \"day\", \"weekday\", \"hour\"]\n",
    "\n",
    "\n",
    "def random_split(df: pd.DataFrame):\n",
    "    x = df.iloc[:, 1:]\n",
    "    y = df.iloc[:, 0]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
    "        x, y, train_size=0.7, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "\n",
    "use_cols = [\n",
    "    'pickup_longitude', \n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude', \n",
    "    'dropoff_latitude', \n",
    "#     'passenger_count', \n",
    "#     'euclidean',\n",
    "    'haversine', \n",
    "    'haversine_bearing', \n",
    "#     'correct_manhattan', \n",
    "    'year', \n",
    "    'month',\n",
    "    'day',\n",
    "    'weekday', \n",
    "    'hour', \n",
    "    'jfk_dist', \n",
    "    'ewr_dist', \n",
    "    'lga_dist',\n",
    "    'sol_dist', \n",
    "    'nyc_dist'\n",
    "]\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = random_split(df_train)\n",
    "x_train = x_train[use_cols]\n",
    "x_val = x_val[use_cols]\n",
    "# del df_train\n",
    "# gc.collect()\n",
    "\n",
    "pool_train = Pool(x_train, y_train, cat_features=cat_feature)\n",
    "pool_val = Pool(x_val, y_val, cat_features=cat_feature)\n",
    "pool_test = Pool(df_test, cat_features=cat_feature)\n",
    "\n",
    "# del x_train, y_train, x_val, y_val\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "pool_train.get_feature_names()\n",
    "\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    iterations=50000,\n",
    "    depth=10,\n",
    "    learning_rate=0.04,\n",
    "    loss_function=\"RMSEWithUncertainty\",\n",
    "    use_best_model=True,\n",
    "    early_stopping_rounds=500,\n",
    "    eval_metric=\"RMSE\",\n",
    "    random_seed=714,\n",
    "    verbose=1000,\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0:1\",\n",
    "    od_type=\"Iter\",\n",
    "    l2_leaf_reg=3.4,\n",
    "    per_float_feature_quantization=['4:border_count=1024'],\n",
    "    random_strength=0.8,\n",
    "    border_count=128,\n",
    "    leaf_estimation_iterations=5\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(pool_train, eval_set=pool_val)\n",
    "\n",
    "\n",
    "y_pred = model.predict(pool_test)\n",
    "\n",
    "data_test['fare_amount'] = y_pred\n",
    "\n",
    "submission = data_test[['key', 'fare_amount']]\n",
    "submission.to_csv(\"submission_20_000_000.csv\", index=False)\n",
    "\n",
    "print(\"Предсказания сохранены!\")\n",
    "# df_out = pd.concat([df_key, pd.Series(y_pred[:,0], name=\"fare_amount\")], axis=1)\n",
    "# df_out.to_csv(\"prediction_30_000_000.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = 'data/new-york-city-taxi-fare-prediction/train_10000000.feather'\n",
    "data_train = pd.read_feather(train_path)\n",
    "\n",
    "# data_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
